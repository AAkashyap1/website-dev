I"$ <p>Visual interesting scene prediction for mobile robots is one of the most fundamental research area in robotic. It is crucial for many robotic applications, such as robot exploration, decision making, and robot coorpertation, etc. Therefore, it is important to establish a sense of “visual interestingness” like humans for mobile robots. To promote the development visual interesting scene prediction for such purpose, we create this dataset for robots to better sense the world.</p>

<h2 id="why-this-data-important">Why this data important?</h2>

<p>This data is recorded in the famous DARPA Challenge 2018-2021, Tunnel Circuit, i.e. the Subterranean (SubT) Challenge.
It is a prize competition funded by the US Defense Advanced Research Projects Agency.
The past DARPA challenges have enabled many state-of-the-arts technologies, including the autonomous driving.
In this challenge, the teams must deploy robotic systems which search and explore subterranean environments.
Since the environments are fully GPS and wireless communication denied, the robots has to do self-localization, pathing planning, exploration, etc., fully autonomously.
We release this data to help the robotists establish a sense of “visual interestingness” for real robots.</p>

<p>In general, the robots has to learn online, detect the ‘interesting’ scenes, and lose interests on repetitive scenes.</p>

<h2 id="who-record-this-data">Who record this data?</h2>

<p>The data is recorded by the <a href="https://www.subt-explorer.com">Team Explorer</a>, who won the first place at the DARPA Subterranean Challenge Tunnel Circuit.
This team was lead by <a href="http://theairlab.org/team/sebastian/">Sebastian Scherer</a>, an associate research professor in the <a href="https://www.ri.cmu.edu">Robotics Institute</a>,<a href="https://www.cmu.edu/">Carnegie Mellon University</a>. <a href="http://theairlab.org/team/sebastian/">Sebastian Scherer</a> is also the director of the <a href="http://theairlab.org">AirLab</a> in CMU.</p>

<h2 id="about-darpa-subterranean-challenge">About DARPA Subterranean Challenge</h2>
<p>The <a href="https://www.subtchallenge.com/">DARPA Subterranean Challenge</a> tasks teams, consisting of university and corporate entities from around the world, to build robotic systems which autonomously search and explore subterranean environments. These environments pose significant challenges to competitors, including a lack of lighting, lack of GPS capabilities, dripping water, thick smoke, and cluttered or irregularly shaped environments. The challenge started in September 2018 and consists of a Systems Track (in which teams compete with physical robots) and a Virtual Track (in which teams compete in the ROS Gazebo virtual simulator). The competition is split into four phases, each capped with a scored challenge event: the Tunnel Circuit (August 2019), which featured an experimental mine in Pittsburgh, PA; the Urban Circuit (February 2020), which features an abandoned nuclear power plant in Seattle, WA; the Cave Circuit (August 2020); and the Final Circuit (August 2021), which will feature elements from previous circuits. On August 2021, DARPA will award a $2 million prize to the winner of the Systems Track and $1.5 million to the winner of the Virtual Track.</p>

<h2 id="data-details">Data details</h2>
<p>Each of the tunnels has a cumulative linear distance of 4-8km.
The SubT front camera (SubTF) dataset listed in the following table, contains seven long videos (1 hour) recorded by two fully autonomous unmanned ground vehicles (UGV).
For better evaluation, we asked multiple volunteers to label the interesting scenes at every 2 seconds so that each sequence is evaluated by at least 3 participants.
It can be seen that SubTF dataset is very challenging, as the human annotation varies a lot, i.e., only 15% and 3.6% of the frames are labeled as interesting by at least 1 (Normal) and 2 subjects (Difficult), respectively.</p>

<table>
  <thead>
    <tr>
      <th>Video</th>
      <th style="text-align: center">Length</th>
      <th style="text-align: right">Normal</th>
      <th style="text-align: right">Difficult</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td style="text-align: center">53.1 min</td>
      <td style="text-align: right">11.11%</td>
      <td style="text-align: right">2.76%</td>
    </tr>
    <tr>
      <td>2</td>
      <td style="text-align: center">55.7 min</td>
      <td style="text-align: right">15.07%</td>
      <td style="text-align: right">4.49%</td>
    </tr>
    <tr>
      <td>3</td>
      <td style="text-align: center">79.4 min</td>
      <td style="text-align: right">9.37%</td>
      <td style="text-align: right">3.02%</td>
    </tr>
    <tr>
      <td>4</td>
      <td style="text-align: center">80.0 min</td>
      <td style="text-align: right">17.51%</td>
      <td style="text-align: right">4.29%</td>
    </tr>
    <tr>
      <td>5</td>
      <td style="text-align: center">59.0 min</td>
      <td style="text-align: right">24.52%</td>
      <td style="text-align: right">4.07%</td>
    </tr>
    <tr>
      <td>6</td>
      <td style="text-align: center">57.5 min</td>
      <td style="text-align: right">22.77%</td>
      <td style="text-align: right">3.30%</td>
    </tr>
    <tr>
      <td>7</td>
      <td style="text-align: center">83.0 min</td>
      <td style="text-align: right">11.04%</td>
      <td style="text-align: right">3.21%</td>
    </tr>
    <tr>
      <td>overall</td>
      <td style="text-align: center">467.7 min</td>
      <td style="text-align: right">15.49%</td>
      <td style="text-align: right">3.58%</td>
    </tr>
  </tbody>
</table>

<p>An important aspect is that the volunteers are expect to only label the first frame, if a continuous frames containing the same scene are interesting.
This means that we expect our robot to report only once, when a interesting scene is found.</p>

<p>Features of this data:</p>
<ul>
  <li>Large scale and Challenging Scenes
    <ul>
      <li>4-8 km mine tunnels</li>
      <li>Dripping water, thick smoke, and cluttered or irregularly shaped environments.</li>
    </ul>
  </li>
  <li>Fully autonomous ground vehicles (AGV).
    <ul>
      <li>Challenging but practical</li>
    </ul>
  </li>
</ul>

<figure>
 <img src="/img/posts/2020-03-01-interestingness-data/map.png" alt="Map" />
 <figcaption>
    One of tunnel maps created by lidar.
 </figcaption>
</figure>

<figure>
 <img src="/img/posts/2020-03-01-interestingness-data/uninteresting.png" alt="uninteresting" />
 <figcaption>
    Some uninteresting scenes.
 </figcaption>
</figure>

<figure>
 <img src="/img/posts/2020-03-01-interestingness-data/sample.png" alt="uninteresting" />
 <figcaption>
    Some interesting scenes.
 </figcaption>
</figure>

<h3 id="download">Download</h3>

<ul>
  <li>Processed Images: <a href="https://entuedu-my.sharepoint.com/:f:/g/personal/cwang017_e_ntu_edu_sg/Evz3OmdaXxtLsbZfwz119jwB0QFmo4huww22dqWUkcw0ng?e=bwC8EF">Link</a></li>
  <li>ROS Bags: <a href="https://entuedu-my.sharepoint.com/:f:/g/personal/cwang017_e_ntu_edu_sg/ErrOwe_Y3MdEq_2Y6xVeiBoBMnclhpUd1V-0F1GnxxgFcQ?e=Vom0M2">Link</a></li>
</ul>

<h3 id="evaluation-tools">Evaluation Tools</h3>

<p>Go to the <a href="https://github.com/wang-chen/SubT">GitHub Repo</a>.</p>

<h3 id="maintenance">Maintenance</h3>

<p><a href="https://chenwang.site">Chen Wang</a> (chenwang[at]dr[dot]com; chenwan3[at]cs[dot]cmu[dot]edu)</p>

<p><a href="http://theairlab.org/team/sebastian/">Sebastian Scherer</a> (basti [at] cmu [dot] edu)</p>

<h3 id="citation">Citation</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  @inproceedings{wang2020visual,
     title={Visual Memorability for Robotic Interestingness via Unsupervised Online Learning},
     author={Wang, Chen and Wang, Wenshan and Qiu, Yuheng and Hu, Yafei and Scherer, Sebastian},
     booktitle={European Conference on Computer Vision (ECCV)},
     year={2020},
  }
</code></pre></div></div>

<p>Download <a href="https://arxiv.org/pdf/2005.08829.pdf">this paper</a>.</p>

<h3 id="acknowledgments">Acknowledgments</h3>

<p>The human subject survey was approved under #2019_00000522.</p>
:ET